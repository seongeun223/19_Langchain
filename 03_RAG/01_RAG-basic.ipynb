{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 문서 로드 (Load Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.12-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.12-cp39-abi3-win_amd64.whl (16.0 MB)\n",
      "   ---------------------------------------- 0.0/16.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 8.4/16.0 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.7/16.0 MB 51.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 33.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.0/16.0 MB 20.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.12\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지 수 : 6\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PYMUPDFLoader 객체 정의\n",
    "loader = PyMuPDFLoader(\"data/snow-white.pdf\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 페이지 수 : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "백설공주\n",
      "옛날어느왕국에공주님이태어났어요.\n",
      "“어쩜이렇게어여쁠까? 살결이눈처럼하얗구나. 백\n",
      "설공주라고불러야겠다.”\n",
      "왕과왕비는갓태어난딸을보며기뻐했어요.\n",
      "하지만기쁨도잠시, 왕비는곧세상을떠나고말았어\n",
      "요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': None, 'metadata': {'source': 'data/snow-white.pdf', 'file_path': 'data/snow-white.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.5', 'title': 'PowerPoint 프레젠테이션', 'author': 'PC', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'creationDate': \"D:20230912112024+09'00'\", 'modDate': \"D:20230912112024+09'00'\", 'trapped': ''}, 'page_content': '백설공주\\n옛날어느왕국에공주님이태어났어요.\\n“어쩜이렇게어여쁠까? 살결이눈처럼하얗구나. 백\\n설공주라고불러야겠다.”\\n왕과왕비는갓태어난딸을보며기뻐했어요.\\n하지만기쁨도잠시, 왕비는곧세상을떠나고말았어\\n요.\\n', 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "# 메타 데이터\n",
    "print(docs[0].__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문서 분할(split Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 청크의 수 : 21\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"분할된 청크의 수 : {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 임베딩 (Embedding 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. DB 생성 (벡터스토어 생성) 및 저장\n",
    "- FAISS(Facebook AI Similarity Search)\n",
    "    - 페이스북에서 개발한 유사도 검색 및 클러스터링 라이브러리\n",
    "    - 벡터 데이터셋에서 빠른 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\20113\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\20113\\miniforge3\\envs\\langchain_env\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 9.2/14.9 MB 47.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 36.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저녁이되자, 일곱난쟁이가돌아왔어요.\n",
      "난쟁이들은쓰러진백설공주를보고엉엉울었어요.\n",
      "백설공주는깊은잠에빠진것처럼보였지요.\n",
      "“백설공주님, 못된왕비의꾐에넘어갔군요.”\n",
      "왕자는깨어난백설공주를보고기뻐했어요.\n",
      "“공주님, 나는이웃나라왕자입니다.”\n",
      "“왕자님이나를다시살려주셨군요.”\n",
      "“나와결혼해주시겠어요?”\n",
      "“네, 좋아요!”\n",
      "간사과랍니다. 잠깐문을열어보세요.”\n",
      "백설공주는고개를저었어요.\n",
      "“난쟁이들이문을열어주지말라고했어요.”\n",
      "백설공주가거절하자, 왕비는창문틈새로사과를쑥내밀었어\n",
      "요.\n",
      "밤이되자오두막주인인일곱난쟁이가돌아왔어요.\n",
      "난쟁이들은집안이어질러진것을보고깜짝놀랐지요.\n",
      "일곱째난쟁이가큰소리로외쳤어요.\n",
      "“누가내침대에서자고있어!”\n"
     ]
    }
   ],
   "source": [
    "for doc in vectorstore.similarity_search(\"난쟁이\"):\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 검색기(Retriever) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어에 있는 정보를 검색하고 생성\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/snow-white.pdf', 'file_path': 'data/snow-white.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.5', 'title': 'PowerPoint 프레젠테이션', 'author': 'PC', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'creationDate': \"D:20230912112024+09'00'\", 'modDate': \"D:20230912112024+09'00'\", 'trapped': ''}, page_content='백설공주\\n옛날어느왕국에공주님이태어났어요.\\n“어쩜이렇게어여쁠까? 살결이눈처럼하얗구나. 백\\n설공주라고불러야겠다.”\\n왕과왕비는갓태어난딸을보며기뻐했어요.'),\n",
       " Document(metadata={'source': 'data/snow-white.pdf', 'file_path': 'data/snow-white.pdf', 'page': 4, 'total_pages': 6, 'format': 'PDF 1.5', 'title': 'PowerPoint 프레젠테이션', 'author': 'PC', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'creationDate': \"D:20230912112024+09'00'\", 'modDate': \"D:20230912112024+09'00'\", 'trapped': ''}, page_content='저녁이되자, 일곱난쟁이가돌아왔어요.\\n난쟁이들은쓰러진백설공주를보고엉엉울었어요.\\n백설공주는깊은잠에빠진것처럼보였지요.\\n“백설공주님, 못된왕비의꾐에넘어갔군요.”'),\n",
       " Document(metadata={'source': 'data/snow-white.pdf', 'file_path': 'data/snow-white.pdf', 'page': 4, 'total_pages': 6, 'format': 'PDF 1.5', 'title': 'PowerPoint 프레젠테이션', 'author': 'PC', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'creationDate': \"D:20230912112024+09'00'\", 'modDate': \"D:20230912112024+09'00'\", 'trapped': ''}, page_content='왕자는백설공주에게반해유리관을달라고부탁했어요.\\n일곱난쟁이는백설공주를잘지킨다는약속을받고유리관을\\n내주었지요.\\n그런데신하들이유리관을옮기다돌부리에툭! 백설공주목\\n에서사과조각이툭!'),\n",
       " Document(metadata={'source': 'data/snow-white.pdf', 'file_path': 'data/snow-white.pdf', 'page': 3, 'total_pages': 6, 'format': 'PDF 1.5', 'title': 'PowerPoint 프레젠테이션', 'author': 'PC', 'subject': '', 'keywords': '', 'creator': 'Microsoft® PowerPoint® 2013', 'producer': 'Microsoft® PowerPoint® 2013', 'creationDate': \"D:20230912112024+09'00'\", 'modDate': \"D:20230912112024+09'00'\", 'trapped': ''}, page_content='간사과랍니다. 잠깐문을열어보세요.”\\n백설공주는고개를저었어요.\\n“난쟁이들이문을열어주지말라고했어요.”\\n백설공주가거절하자, 왕비는창문틈새로사과를쑥내밀었어\\n요.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"백설공주와 일곱난쟁이는 어디서 만났어?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 프롬프트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당신은 질문 - 답변 작업을 위한 어시스턴트입니다.  \n",
    "주어진 문맥을 사용하여 질문에 답변하세요.  \n",
    "유치원 선생님이 아이에게 말하는 것처럼 매우 친절하고 부드러운 어조를 사용하세요.  \n",
    "따듯하고 친근한 방식으로 말하세요.  \n",
    "답을 모르는 경우에는 모른다고 말씀하세요.  \n",
    "한국어로 답변하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "Use a very kind and gentle tone like a kindergarten teacher talking to a child.\n",
    "Speak in a warm and friendly way.\n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# RunnablePassthrough 그대로 전달해주는 것 \n",
    "\n",
    "chain = (\n",
    "    {\"context\" : retriever, \"question\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미안하지만, 제일 큰 난쟁이가 누구인지에 대한 정보는 여기 없어요. 하지만 괜찮아요, 다른 재미있는 이야기를 찾아볼 수 있답니다!\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행\n",
    "\n",
    "question = \"제일 큰 난쟁이는 누구야?\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 페이지 수 : 25\n",
      "분할된 청크의 수 : 499\n",
      "보고서는 AI 국제과학 패널 구성, AI 정책 대화, AI 표준 교류, AI 역량 개발 네트워크 형성 등 글로벌 AI 거버넌스의 격차 해소를 위한 7가지 권고안을 제시하고 있습니다. 또한, AI의 추론 역량이 물리학, 화학, 생물학의 복잡한 과제에서 박사과정 학생과 비슷한 성과를 나타내며, 수학과 코딩에서도 탁월한 성과를 기록하고 있습니다. 다만, 다국어 작업에는 최적화되지 않아 추가적인 미세조정이 필요하다는 점도 언급하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문서 로드\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# PYMUPDFLoader 객체 정의\n",
    "loader = PyMuPDFLoader(\"data/SPRi AI Brief_10.pdf\")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 페이지 수 : {len(docs)}\")\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"분할된 청크의 수 : {len(split_documents)}\")\n",
    "\n",
    "# 임베딩\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "    \n",
    "# 벡터스토어에 있는 정보를 검색하고 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# RunnablePassthrough 그대로 전달해주는 것 \n",
    "\n",
    "chain = (\n",
    "    {\"context\" : retriever, \"question\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 체인 실행\n",
    "\n",
    "question = \"대략적으로 전체적인 내용을 요약해줘\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모르겠습니다. 주어진 문맥에서는 아파트가 인기 있는 이유에 대한 정보가 포함되어 있지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "\n",
    "# 환경 변수 가져오기\n",
    "load_dotenv()\n",
    "\n",
    "# 문서 로드\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://n.news.naver.com/article/437/0000416134\"),\n",
    "    bs_kwargs=dict(\n",
    "        # 특정 요소에서만 파싱하도록 \n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\" : [\"newsct_article _article_body\", \"media_end_head go_trans\"]}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 임베딩 생성\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# 검색기 (retriever) 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# runtime\n",
    "\n",
    "# 프롬프트\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    당신은 질문-답변을 수행하는 AI 어시스턴트이다.\n",
    "    주어진 문맥에 검색된 다음 문맥(context)를 사용해 질문에 답해야 한다.\n",
    "    만약 주어진 문맥에서 답을 찾을 수 없는 경우, 모른다고 이야기 하세요.\n",
    "    한글로 답변해주세요.\n",
    "    \n",
    "    #Question:\n",
    "    {question}\n",
    "    \n",
    "    #Context:\n",
    "    {context}\n",
    "    \n",
    "    #Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# LLM 모델\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Chain 구성\n",
    "news_chain = (\n",
    "    {\"context\":retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 질문\n",
    "answer = news_chain.invoke(\"아파트가 왜 인기 있는 거야?\")\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
